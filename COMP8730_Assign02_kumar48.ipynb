{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb509551",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82199100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os\n",
    "import nltk\n",
    "from nltk.util import pad_sequence\n",
    "from nltk.util import bigrams\n",
    "from nltk.util import ngrams\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import pad_both_ends\n",
    "from nltk.lm.preprocessing import flatten\n",
    "from nltk.corpus import brown, reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02bd814",
   "metadata": {},
   "source": [
    "### Loading APPLING1DAT.643 as corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06689906",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data/0643/APPLING1DAT.643\"\n",
    "with open(data_path) as fp:\n",
    "    data = fp.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618fdd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$Punjabi\\nstrang  strange  I felt very *\\nbrake  break  at * time\\nbrack  break  wh'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dbfb2d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path=\"data/0643/APPLING1DAT.643\"):\n",
    "    spell_corpus = []\n",
    "    inp = []\n",
    "    crct_ans = []\n",
    "    with open(data_path) as fp:\n",
    "        data = fp.read()\n",
    "    for line in data.split('\\n'):\n",
    "        w = line.split()\n",
    "        if len(w)>2:\n",
    "            crct_ans.append(w[1])\n",
    "            inp_ = \" \".join(w[2:]).strip()\n",
    "            inp.append(inp_)\n",
    "            spell_corpus.append(re.sub(r\"\\*\", w[1], inp_))\n",
    "    return spell_corpus, inp, crct_ans\n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "025beb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_corpus, inp, crct_ans = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98905bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I felt very strange',\n",
       " 'at break time',\n",
       " 'when the break was finished',\n",
       " 'in the winter when it was snowing',\n",
       " 'I thought it was a ghost']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c341e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_qaulity(corpus, word_interest):\n",
    "    corpora = {'brown': brown, 'reuters': reuters}\n",
    "    corp = corpora.get(corpus, brown)\n",
    "    print(f'Inspecting {corpus} corpus')\n",
    "    \n",
    "    combined = {k:0 for k in word_interest}\n",
    "    for cat in corp.categories():\n",
    "\n",
    "        cat_words = corp.words(categories=cat)\n",
    "        fdist = nltk.FreqDist(w.lower() for w in cat_words)\n",
    "        t = 0\n",
    "        for m in word_interest:\n",
    "            print(m + ':', fdist[m], end=' ')\n",
    "            t+=fdist[m]\n",
    "            combined[m] += fdist[m]\n",
    "        print(\"\\n\")\n",
    "        print(f'{cat} -> {t}')\n",
    "        print(\"=\"*40)\n",
    "    print(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820b6ed",
   "metadata": {},
   "source": [
    "### Selecting Primary Corpus for LM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d32958c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether the correct spelling from our corpus E is there in the train data or not\n",
    "# dropped the plan as there was not much difference in result and difficult to report in single page format\n",
    "# check_data_qaulity(\"brown\", crct_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5ed56e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prepfn = lambda all_data: [[w.lower() if w.isalpha() else w for w in sent if w.isalpha() or w.strip()=='.' or w.strip()==','] for sent in all_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e895bae1",
   "metadata": {},
   "source": [
    "### Final Experiment Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7fe3b70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment brown-newsK with data brown-news\n",
      "Loading saved model on disk\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "on and * off\n",
      "5\n",
      "on and \n",
      "['off', 'the', 'a']\n",
      "(1.0, 1.0, 1.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "on and * off\n",
      "10\n",
      "on and \n",
      "['off', 'the', 'a']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 1.0)\n",
      "to * people scared make\n",
      "5\n",
      "to \n",
      "['the', 'be', 'a', 'get', 'have', 'make', 'take', 'do', 'see', 'go']\n",
      "(0.0, 0.0, 1.0)\n",
      "(0.0, 0.0, 1.0)\n",
      "to * people scared make\n",
      "10\n",
      "to \n",
      "['the', 'be', 'a', 'get', 'have', 'make', 'take', 'do', 'see', 'go']\n",
      "(0.0, 0.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 1.0)\n",
      "the things I * believe\n",
      "5\n",
      "the things i \n",
      "['think', 'was', 'am', 'have', 'had', 'can', 'know', 'believe', 'hope', 'feel']\n",
      "(0.0, 0.0, 1.0)\n",
      "(0.0, 0.0, 1.0)\n",
      "the things I * believe\n",
      "10\n",
      "the things i \n",
      "['think', 'was', 'am', 'have', 'had', 'can', 'know', 'believe', 'hope', 'feel']\n",
      "(0.0, 0.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "they * cleaned were\n",
      "5\n",
      "they \n",
      "['are', 'have', 'were', 'had', 'will', 'would', 'can', 'could', 'also', 'did']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "they * cleaned were\n",
      "10\n",
      "they \n",
      "['are', 'have', 'were', 'had', 'will', 'would', 'can', 'could', 'also', 'did']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "the most * player famous\n",
      "5\n",
      "the most \n",
      "['recent', 'famous', 'valuable', 'serious', 'ambitious', 'positive', 'difficult', 'populous', 'impressive', 'troublesome']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "the most * player famous\n",
      "10\n",
      "the most \n",
      "['recent', 'famous', 'valuable', 'serious', 'ambitious', 'positive', 'difficult', 'populous', 'impressive', 'troublesome']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "what had * happened\n",
      "5\n",
      "what had \n",
      "['happened']\n",
      "(1.0, 1.0, 1.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "what had * happened\n",
      "10\n",
      "what had \n",
      "['happened']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "what had * happened\n",
      "5\n",
      "what had \n",
      "['happened']\n",
      "(1.0, 1.0, 1.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "what had * happened\n",
      "10\n",
      "what had \n",
      "['happened']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 1.0)\n",
      "I * it was believe\n",
      "5\n",
      "i \n",
      "['think', 'was', 'am', 'have', 'had', 'can', 'know', 'believe', 'hope', 'feel']\n",
      "(0.0, 0.0, 1.0)\n",
      "(0.0, 0.0, 1.0)\n",
      "I * it was believe\n",
      "10\n",
      "i \n",
      "['think', 'was', 'am', 'have', 'had', 'can', 'know', 'believe', 'hope', 'feel']\n",
      "(0.0, 0.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "{5: [0.015151515151515152, 0.025252525252525252, 0.04040404040404041], 10: [0.015151515151515152, 0.025252525252525252, 0.04040404040404041]}\n"
     ]
    }
   ],
   "source": [
    "data = brown.sents(categories=\"news\") # Corpus C on which LM will be trained\n",
    "avgk, len_tseed = experiment(\"brown-newsK\", data, \"brown-news\") \n",
    "# arg1->folder_name(save and load models from);arg2-> dataset;arg3->dataname for debugging\n",
    "# also have implementation for bi-direction n-gram but as product of 2 LMs and is not giving good result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837f9ff",
   "metadata": {},
   "source": [
    "### Execute below codes to play with the code in above cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4341e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_modelK(out_, data_prep):\n",
    "#     out_ = \"ngrams_Brown\"\n",
    "    if not os.path.exists(out_): os.mkdir(out_)\n",
    "    n_model = []\n",
    "    n_l = [1, 2, 3, 5, 10]\n",
    "    for n in n_l:\n",
    "        train_data, padded_sents = padded_everygram_pipeline(n, data_prep)\n",
    "        model = KneserNeyInterpolated(n) \n",
    "#         print(f'{n}-gram: With {model.vocab} vocab')\n",
    "        model.fit(train_data, padded_sents)\n",
    "        print(f'Training Complete for {n}-gram {model.vocab}')\n",
    "    #     t_se = \"<s> my friends, joan and arthur live near the\".lower()\n",
    "    #     generate_topk(model, k=10, text_seed=word_tokenize(t_se))\n",
    "        n_model.append(model)\n",
    "        with open(f'{out_}/{n}-grams.pkl', 'wb') as fout:\n",
    "            pickle.dump(model, fout)\n",
    "    return n_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "71952a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_model(out_, data_prep):\n",
    "#     out_ = \"ngrams_Brown\"\n",
    "    if not os.path.exists(out_): os.mkdir(out_)\n",
    "    n_model = []\n",
    "    n_l = [1, 2, 3, 5, 10]\n",
    "    for n in n_l:\n",
    "        train_data, padded_sents = padded_everygram_pipeline(n, data_prep)\n",
    "        model = MLE(n) \n",
    "#         print(f'{n}-gram: With {model.vocab} vocab')\n",
    "        model.fit(train_data, padded_sents)\n",
    "        print(f'Training Complete for {n}-gram {model.vocab}')\n",
    "    #     t_se = \"<s> my friends, joan and arthur live near the\".lower()\n",
    "    #     generate_topk(model, k=10, text_seed=word_tokenize(t_se))\n",
    "        n_model.append(model)\n",
    "        with open(f'{out_}/{n}-grams.pkl', 'wb') as fout:\n",
    "            pickle.dump(model, fout)\n",
    "    return n_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "74fb5cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d885a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_spell(inp, crct_ans, n_model):    \n",
    "    \n",
    "    if type(n_model)==str:\n",
    "        models = []\n",
    "        fnm = [f'{i}-grams.pkl' for i in [5, 10]]\n",
    "        for f in fnm:\n",
    "            with open(f\"{n_model}/{f}\", \"rb\") as fp:\n",
    "                models.append(pickle.load(fp))\n",
    "        n_model = models\n",
    "        \n",
    "    len_tseed = []\n",
    "    \n",
    "    score = {k.order:[0, 0, 0] for k in n_model}\n",
    "        \n",
    "    for i, (in_, ans_) in enumerate(zip(inp, crct_ans)):\n",
    "        sear = re.search(r\"\\*\", in_)\n",
    "        if sear:\n",
    "            ix = sear.start()\n",
    "        else:\n",
    "            ix = len(in_)\n",
    "            print(\"Fracko\")\n",
    "        t_se = in_[:ix].lower()\n",
    "        len_tseed.append(len(t_se.split()))\n",
    "        \n",
    "        for model in n_model:\n",
    "            topk = generate_topk(model, k=10, text_seed=word_tokenize(t_se))\n",
    "            avgk = report_successk(ans_, topk)\n",
    "            print(avgk)\n",
    "            for idx, succ_k in enumerate(avgk):\n",
    "                score[model.order][idx]+=succ_k \n",
    "            \n",
    "            if any(avgk):\n",
    "                print(in_, ans_)\n",
    "                print(model.order)\n",
    "                print(t_se)\n",
    "                print(topk)\n",
    "                print(avgk)\n",
    "    return {k:[elm/len(inp) for elm in v] for k, v in score.items()}, len_tseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f435d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_spell_bi(inp, crct_ans, n_model, n_bmodel):    \n",
    "    \n",
    "    if type(n_model)==str:\n",
    "        models = []\n",
    "        fnm = [f'{i}-grams.pkl' for i in [1, 2, 3]]\n",
    "        for f in fnm:\n",
    "            with open(f\"{n_model}/{f}\", \"rb\") as fp:\n",
    "                models.append(pickle.load(fp))\n",
    "        n_model = models\n",
    "        \n",
    "    len_tseed = []\n",
    "    len_tseed_rev = []\n",
    "    \n",
    "    score = {k.order:[0, 0, 0] for k in n_model}\n",
    "        \n",
    "    for i, (in_, ans_) in enumerate(zip(inp, crct_ans)):\n",
    "        sear = re.search(r\"\\*\", in_)\n",
    "        in_r = in_[::-1]\n",
    "        sear_rev = re.search(r\"\\*\", in_r)\n",
    "        if sear:\n",
    "            ix = sear.start()\n",
    "            ix_rev = sear_rev.start()\n",
    "        else:\n",
    "            ix = len(in_)\n",
    "            ix_rev = len(in_)\n",
    "            print(\"Fracko\")\n",
    "        \n",
    "        t_se = in_[:ix].lower()\n",
    "        t_se_rev = in_r[:ix_rev].lower()\n",
    "        \n",
    "        len_tseed.append(len(t_se.split()))\n",
    "        len_tseed_rev.append(len(t_se_rev.split()))\n",
    "        \n",
    "        for model, bmodel in zip(n_model, n_bmodel):\n",
    "            topk = generate_topkbi(model, bmodel, k=10, text_seed=word_tokenize(t_se), text_seed_rev=word_tokenize(t_se_rev))\n",
    "            avgk = report_successk(ans_, topk)\n",
    "            print(avgk)\n",
    "            for idx, succ_k in enumerate(avgk):\n",
    "                score[model.order][idx]+=succ_k \n",
    "            if any(avgk):\n",
    "                print(in_, ans_)\n",
    "                print(model.order)\n",
    "                print(t_se)\n",
    "                print(topk)\n",
    "                print(avgk)\n",
    "    return {k:[elm/len(inp) for elm in v] for k, v in score.items()}, len_tseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e6a01da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topk(model, k, text_seed=None):\n",
    "    text_seed = [] if text_seed is None else list(text_seed)\n",
    "    # This is the base recursion case.\n",
    "    context = (\n",
    "        text_seed[-model.order + 1 :]\n",
    "        if len(text_seed) >= model.order\n",
    "        else text_seed\n",
    "    )\n",
    "    samples = model.context_counts(model.vocab.lookup(context))\n",
    "    while context and not samples:\n",
    "        context = context[1:] if len(context) > 1 else []\n",
    "        samples = model.context_counts(model.vocab.lookup(context))\n",
    "    t_ = sorted(samples.items(), key=lambda x: x[1], reverse=True)\n",
    "#     print(t_[:k])\n",
    "    samples = dict(t_)\n",
    "    return list(samples.keys())[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b00854b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import os\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.lm import MLE, Laplace, KneserNeyInterpolated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dbb23017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_topkbi(model, bmodel, k=10, text_seed=None, text_seed_rev=None):\n",
    "    text_seed = [] if text_seed is None else list(text_seed)\n",
    "    text_seed_rev = [] if text_seed_rev is None else list(text_seed_rev)\n",
    "    # This is the base recursion case.\n",
    "    context = (text_seed[-model.order + 1 :] if len(text_seed) >= model.order else text_seed)\n",
    "    context_rev = (text_seed_rev[-model.order + 1 :] if len(text_seed_rev) >= model.order else text_seed_rev)\n",
    "    samples = model.context_counts(model.vocab.lookup(context))\n",
    "    samples_rev = bmodel.context_counts(bmodel.vocab.lookup(context_rev))\n",
    "    \n",
    "    while context and not samples:\n",
    "        context = context[1:] if len(context) > 1 else []\n",
    "        samples = model.context_counts(model.vocab.lookup(context))\n",
    "    \n",
    "    while context_rev and not samples_rev:\n",
    "        context_rev = context_rev[1:] if len(context_rev) > 1 else []\n",
    "        samples_rev = bmodel.context_counts(bmodel.vocab.lookup(context_rev))\n",
    "\n",
    "    samples.update(samples_rev)\n",
    "    \n",
    "    t_ = sorted(samples.items(), key=lambda x: x[1], reverse=True)\n",
    "    samples = dict(t_)\n",
    "    return list(samples.keys())[:k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6fd0852",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experimentbiDir(name, data, data_name):\n",
    "    print(f\"Running experiment {name} with data {data_name}\")\n",
    "    if os.path.exists(name):\n",
    "        if len(os.listdir(name))>1:\n",
    "            print(f\"Loading saved model on disk\")\n",
    "            models = []\n",
    "            fnm = [f'{i}-grams.pkl' for i in [1, 2, 3]]\n",
    "            for f in fnm:\n",
    "                with open(f\"{name}/{f}\", \"rb\") as fp:\n",
    "                    models.append(pickle.load(fp))\n",
    "            n_model = models\n",
    "        if len(os.listdir(f'{name}_rev'))>1:\n",
    "            print(f\"Loading saved model on disk\")\n",
    "            models = []\n",
    "            fnm = [f'{i}-grams.pkl' for i in [1, 2, 3, 5, 10]]\n",
    "            for f in fnm:\n",
    "                with open(f\"{name}_rev/{f}\", \"rb\") as fp:\n",
    "                    models.append(pickle.load(fp))\n",
    "            n_bmodel = models\n",
    "            avgk, len_tseed = evaluate_spell_bi(inp, crct_ans, n_model, n_bmodel)\n",
    "    else:\n",
    "        n_model = []\n",
    "        n_bmodel = []\n",
    "        \n",
    "        data_prep = data_prepfn(data)\n",
    "        data_rev = [elem[::-1] for elem in data]\n",
    "        data_prep_rev = data_prepfn(data_rev)\n",
    "        print(f\"Training and saving Frwd model on disk\")\n",
    "        n_model = dump_modelK(name, data_prep)\n",
    "        print(f\"Training and saving Reverse model on disk\")\n",
    "        n_bmodel = dump_modelK(f'{name}_rev', data_prep_rev)\n",
    "        avgk, len_tseed = evaluate_spell_bi(inp, crct_ans, n_model, n_bmodel)\n",
    "    print(avgk)\n",
    "    return avgk, len_tseed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "757d4f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pytrec_eval\n",
    "def report_successk(crctspell, topkspell):\n",
    "    qrel = {\n",
    "        'q1': {\n",
    "            f'{crctspell}': 1\n",
    "        }\n",
    "    }\n",
    "\n",
    "    run = {\n",
    "        'q1': {k:v for k,v in zip(topkspell, list(range(-1,-(len(topkspell)+1), -1)))}\n",
    "    }\n",
    "#     print(f\"{qrel}\\n{run}\")\n",
    "    evaluator = pytrec_eval.RelevanceEvaluator(qrel, {'success_1', 'success_5', 'success_10'})\n",
    "    eval_ = evaluator.evaluate(run)['q1']\n",
    "#     print(eval_)\n",
    "    return eval_['success_1'], eval_['success_5'], eval_['success_10']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "34c3dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(name, data, data_name):\n",
    "    print(f\"Running experiment {name} with data {data_name}\")\n",
    "    if os.path.exists(name):\n",
    "        if len(os.listdir(name))>1:\n",
    "            print(f\"Loading saved model on disk\")\n",
    "            avgk, len_tseed = evaluate_spell(inp, crct_ans, name)\n",
    "    else:\n",
    "        n_model = []\n",
    "        \n",
    "        data_prep = data_prepfn(data)\n",
    "        print(f\"Training and saving model on disk\")\n",
    "        n_model = dump_modelK(name, data_prep)\n",
    "        avgk, len_tseed = evaluate_spell(inp, crct_ans, n_model)\n",
    "    print(avgk)\n",
    "    return avgk, len_tseed\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56b8fbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc95a85",
   "metadata": {},
   "source": [
    "data = brown.sents(categories=\"news\")\n",
    "avgk, len_tseed = experiment(\"brown-newsK\", data, \"brown-news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "00aef1bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0.0,\n",
       " 2: 0.0101010101010101,\n",
       " 3: 0.026936026936026935,\n",
       " 5: 0.026936026936026935,\n",
       " 10: 0.026936026936026935}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## brown news       \n",
    "{1: 0.0, 2: 0.0101010101010101, 3: 0.026936026936026935, 5: 0.026936026936026935, 10: 0.026936026936026935}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e515ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\"belles_lettres\", \"learned\"\n",
    "# {1: 0.0, 2: 0.011784511784511785, 3: 0.021885521885521883, 5: 0.021885521885521883, 10: 0.021885521885521883}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c95b526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brown-all\n",
    "{1: 0.0, 2: 0.010101010101010102, 3: 0.02693602693602694}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4667e5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# brown-news Lap\n",
    "{1: 0.0, 2: 0.0101010101010101, 3: 0.026936026936026935, 5: 0.026936026936026935, 10: 0.026936026936026935}\n",
    "# brow-news Key\n",
    "{1: 0.0, 2: 0.0101010101010101, 3: 0.026936026936026935, 5: 0.026936026936026935, 10: 0.026936026936026935}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05033f52",
   "metadata": {},
   "source": [
    "## Running distil bert for the same task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c947ce6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['role', 'fashion', 'business', 'model', 'modeling']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "unmasker = pipeline('fill-mask', model='distilbert-base-uncased')\n",
    "resp = unmasker(\"Hello I'm a [MASK] model.\")\n",
    "topk = [elm['token_str']for elm in resp]\n",
    "topk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "883dc8f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "in the * when it was snowing winter\n",
      "['winter', 'summer', 'morning', 'mornings', 'afternoon']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "everything * the houses except\n",
      "['in', 'except', 'around', 'inside', 'but']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "the wind * the leaves blew\n",
      "['blows', 'carries', 'shakes', 'blew', 'rustling']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "making any * noise\n",
      "['difference', '.', 'noise', 'progress', 'decisions']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "an * in his hand arrow\n",
      "['arrow', 'envelope', 'apple', 'axe', 'object']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "they * a hole dig\n",
      "['dug', 'dig', 'found', 'cut', 'blew']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "a * of bait piece\n",
      "['bunch', 'piece', 'lot', 'nest', 'bowl']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "they throw * stones\n",
      "['stones', '.', ';', ':', '!']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "three * people hundred\n",
      "['thousand', 'hundred', 'million', 'dozen', 'billion']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "hit him with a * hammer\n",
      "['!', '.', 'hammer', '?', 'fist']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "a * of lemonade bottle\n",
      "['bottle', 'glass', 'slice', 'bowl', 'taste']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "on and * off\n",
      "['.', '!', 'off', ';', '?']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "all of a * sudden\n",
      "['sudden', 'kind', '.', '!', 'feather']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "I was * of it scared\n",
      "['proud', 'afraid', 'sick', 'sure', 'scared']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "* the window through\n",
      "['open', 'through', 'above', 'behind', 'opening']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "to * people scared make\n",
      "['keep', 'scare', 'make', 'get', 'leave']\n",
      "(0.0, 1.0, 1.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "lit a * candle\n",
      "['candle', '.', 'torch', 'lamp', 'cigarette']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "something * happened bad\n",
      "['terrible', 'bad', 'awful', 'horrible', 'strange']\n",
      "(0.0, 1.0, 1.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "* and lightning thunder\n",
      "['thunder', 'hail', 'fire', 'stars', 'rain']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "* do you mean what\n",
      "['what', 'how', 'why', 'where', 'who']\n",
      "(1.0, 1.0, 1.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "I'm * a letter writing\n",
      "['writing', 'sending', 'getting', 'reading', 'holding']\n",
      "(1.0, 1.0, 1.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "a * line straight\n",
      "['straight', 'fault', 'branch', 'spur', 'dividing']\n",
      "(1.0, 1.0, 1.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "I * it was going to rain thought\n",
      "['thought', 'knew', 'think', 'guess', 'swear']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "he just * to die wanted\n",
      "['wants', 'wanted', 'deserved', 'deserves', 'needs']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "fried rice and * noodles\n",
      "['beans', 'rice', 'vegetables', 'chicken', 'noodles']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "carry money * me with\n",
      "['on', 'for', 'with', 'to', 'around']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "to * out find\n",
      "['find', 'seek', 'reach', 'make', 'get']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "the roof is being * repaired\n",
      "['restored', 'repaired', 'rebuilt', 'renovated', 'renewed']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "they * cleaned were\n",
      "['are', 'were', 'have', 'always', 'never']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "the man had * to smuggle tried\n",
      "['managed', 'tried', 'attempted', 'begun', 'learned']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(1.0, 1.0, 1.0)\n",
      "he'd * fire to set\n",
      "['set', 'opened', 'lit', 'breathe', 'open']\n",
      "(1.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "what had * happened\n",
      "['?', 'happened', '!', '.', 'become']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "he'd * the woman saved\n",
      "['kissed', 'killed', 'seen', 'met', 'saved']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "what had * happened\n",
      "['?', 'happened', '!', '.', 'become']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "gamblers play * poker\n",
      "['.', '!', ';', '?', 'poker']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "it is without * doubt\n",
      "['.', 'borders', 'doubt', 'trace', 'bounds']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "* countries developing\n",
      "['member', 'nordic', 'host', 'participating', 'developing']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "my * hurt stomach\n",
      "['heart', 'stomach', 'head', 'eyes', 'knee']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "an * hot day extremely\n",
      "['unusually', 'exceptionally', 'extremely', 'incredibly', 'intensely']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "I never * that thought\n",
      "['forgot', 'dreamed', 'forget', 'thought', 'imagined']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 1.0, 1.0)\n",
      "I'd * that heard\n",
      "['forgotten', 'learned', 'heard', 'hoped', 'said']\n",
      "(0.0, 1.0, 1.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "(0.0, 0.0, 0.0)\n",
      "{1: 0.06565656565656566, 5: 0.20707070707070707, 10: 0.20707070707070707}\n"
     ]
    }
   ],
   "source": [
    "sum_ = [0, 0, 0]\n",
    "for i, (in_, ans_) in enumerate(zip(inp, crct_ans)):\n",
    "    new_inp = re.sub(\"\\*\", '[MASK]', in_)\n",
    "    resp = unmasker(new_inp)\n",
    "    topk = [elm['token_str']for elm in resp]\n",
    "    avgk = report_successk(ans_, topk)\n",
    "    print(avgk)\n",
    "    for idx, s_k in enumerate(avgk):\n",
    "        sum_[idx]+=s_k\n",
    "    if any(avgk):\n",
    "        print(in_, ans_)\n",
    "        print(topk)\n",
    "        print(avgk)\n",
    "succ_k = {k:v/len(inp) for k, v in zip([1, 5, 10], sum_)}\n",
    "print(succ_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformer\n",
    "# {1: 0.06565656565656566, 5: 0.20707070707070707, 10: 0.20707070707070707}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
